Group Members:
Derek Foster    504146063
Rebecca Hubeny	104133091

Slip Time:
(i) This project is late by 1 day.
(ii) Both partners now have 2 remaining slip days.

LAB 3 REPORT

Parser.java -- Exercise 1

Step 1: simpledb.Parser.main() and simpledb.Parser.start() simpledb.Parser.main() is the entry point for the SimpleDB system. It calls simpledb.Parser.start(). The latter performs three main actions:

1. It populates the SimpleDB catalog from the catalog text file provided by the user as argument (Database.getCatalog().loadSchema(argv[0]);).
2. For each table defined in the system catalog, it computes statistics over the data in the table by calling: TableStats.computeStatistics(), which then does: TableStats s = new TableStats(tableid, IOCOSTPERPAGE);
3. It processes the statements submitted by the user (processNextStatement(new ByteArrayInputStream(statementBytes));)

Step 2: simpledb.Parser.processNextStatement() 
This method takes two key actions:
1. First, it gets a physical plan for the query by invoking handleQueryStatement((ZQuery)s);
2. Then it executes the query by calling query.execute();

Step 3: simpledb.Parser.handleQueryStatement() 
1. First this function sets the query's logical and physical plans by LogicalPlan lp = parseQueryLogicalPlan(tId, s); and lp.physicalPlan(tId, TableStats.getStatsMap(), explain);. To find the physical plan it had to find the logical plan first, hence the lp.physicalPlan. Once found, the plans are set to that query.
2. If there actually is a physical plan, it gets the method to update the Operator Cardinality. It then invokes this method, as seen in m.invoke(null, (Operator) physicalPlan, lp.getTableAliasToIdMapping(), TableStats.getStatsMap());

Step 4:

The implementation of IntHistogram.java was not overwhelmingly difficult, 
but it did cause a few headaches with how specific it was. The size of a 
bucket was needed, so we had a member variable m_bucketSize fort this, 
although it was giving us issues with dividing by 0. We resolved this by
realizing we need to cast both the denominator and numerator before 
dividing so we get an actual fraction, not a rounded off integer. We 
implemented a few helper functions to assist in adding a value to the 
histogram, which is simply an array of ints. The helpers were used more in
estimateSelectivity, which the majority of the work and even some problems 
with TableStats.java occurred.

estimateSelectivity is broken down into each possible operation, and then 
handled accordingly. Each involved some trial and error, but NOT_EQUALS 
gave us a lot of trouble in terms of TableStats.java; all of the tests in 
TableStats.java worked, except for estimateSelectivity. After narrowing 
down which specifc assert cases, we found that it was the NOT_EQUALS cases
that were not being executed as they should have. To fix this, NOT_EQUALS
was implemented in its own if statement, and its results were returned 
immediately to avoid any miscalculations.

The constructor of TableStats was also a point of hardship. We ended up 
deciding that HashMaps were the best way to design this class, since we 
have used them before in SimpleDB and also they were used in this specific
class prior to us filling our portion of the code. In essence, we found the
transaction ID to get an iterator to help catch exceptions and also to get
values we needed. We start by getting the max and min values in separate 
HashMaps, and then proceed on to the histograms, adding values according to 
if they are INT_TYPE or STRING_TYPE. The histogram is then added to our 
HashMap, and we are able to use the HashMap to find different stats, such 
as selectivity.

Since we used a nested loop join, implementing estimateJoinCost() was
simply a matter of applying the formula in the lab manual. The method
estimateJoinCardinality was also simple because only a few lines of code
where necessary to implement the requirements listed in the spec.

The method orderJoins() took a little longer because we needed to
read through more information, such as the provided slides and helper
methods and classes. However, the implementation process itself was
short because the psuedocode already outlined the algorithm. We used the
methods computeCostAndCardOfSubplan() and printJoins() as well as the
classes CostCard and PlanCache to implement orderJoins(). The algorithm
makes use of dynamic programming to calculate the optimal order of
smaller subsets, increasing the size of the subsets until we find the
optimal order.

Exercise 6 required us to run the query:
	select d.fname, d.lname
	from Actor a, Casts c, Movie_Director m, Director d
	where a.id=c.pid and c.mid=m.mid and m.did=d.id 
	and a.fname='John' and a.lname='Spicer';
The resulting query plan was as follows:
                            π(d.fname,d.lname),card:29729
                            |
                            ⨝(a.id=c.pid),card:29729
  __________________________|___________________________
  |                                                    |
  σ(a.lname=Spicer),card:1                             ⨝(m.mid=c.mid),card:29729
  |                                    ________________|_________________
  σ(a.fname=John),card:1               |                                |
  |                                    ⨝(d.id=m.did),card:2791          |
  |                           _________|_________                       |
  |                           |                 |                     scan(Casts c)
scan(Actor a)               scan(Director d)  scan(Movie_Director m)

d.fname	d.lname	

We used the 1% version of the IMDB database. The optimizer looked at the
estimated cost of joining any two of a, c, m, and d. It recorded the minimum
cost plan of joining each subset. It then looked at the cost of joining a subset of
size 3 using the minimum cost plan of each size 2 subset as a base cost. The minimum
cost plan of each size 3 subset was then recorded. Finally, the optimizer used these
minimum cost plans of size 3 subsets to find the minimum cost plan of the size 4 subset.
We see in the above query plan, that the minimum cost plan joins a scan of table d
and a scan of table m. This is then joined with a scan of table c. Table a is then
joined to find the results. The optimizer chose this plan because there was no
cheaper way, based on our estimations, to form this set of 4.

We also executed another SQL query as written below:
	select a.fname, a.lname
	from Actor a, Casts c, Movie m, Genre g
	where a.id=c.pid and c.mid=m.id and m.id=g.mid and g.genre ='Western';
The optimizer generated the following query plan:
                           π(a.fname,a.lname),card:1
                           |
                           ⨝(g.mid=m.id),card:1
  _________________________|_________________________
  |                                                 |
  σ(g.genre=Western),card:1                         ⨝(c.pid=a.id),card:26026
  |                                   ______________|______________
  |                                   |                           |
  |                                   ⨝(m.id=c.mid),card:29729    |
  |                            _______|________                   |
  |                            |              |                 scan(Actor a)
scan(Genre g)                scan(Movie m)  scan(Casts c)

a.fname	a.lname	

The optimizer generates this plan because a scan of m joined with c in the
indicated outer loop is the fastest way to join m and c. This is then joined
with a because it is the fastest way to form a joined table of m, c, and a.
g is joined next because it is the fastest estimated way to form a joined
table of m, c, a, and d. The optimizer used dynamic programming to find
each best plan.

This lab took us a bit longer than the other two labs, as the concepts in 
this lab are much more advanced. The detailed nature of this lab, such as 
calculating table statistics, took a lot longer than we expected, hence our
project being late. Overall, the lab took us about a week and a half, almost
2 weeks.
