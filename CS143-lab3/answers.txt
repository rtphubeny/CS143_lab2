Group Members:
Derek Foster    504146063
Rebecca Hubeny	104133091

Slip Time:
(i) This project is late by 1 day.
(ii) Both partners now have 2 remaining slip days.

LAB 3 REPORT

Parser.java

The implementation of IntHistogram.java was not overwhelmingly difficult, 
but it did cause a few headaches with how specific it was. The size of a 
bucket was needed, so we had a member variable m_bucketSize fort this, 
although it was giving us issues with dividing by 0. We resolved this by
realizing we need to cast both the denominator and numerator before 
dividing so we get an actual fraction, not a rounded off integer. We 
implemented a few helper functions to assist in adding a value to the 
histogram, which is simply an array of ints. The helpers were used more in
estimateSelectivity, which the majority of the work and even some problems 
with TableStats.java occurred.

estimateSelectivity is broken down into each possible operation, and then 
handled accordingly. Each involved some trial and error, but NOT_EQUALS 
gave us a lot of trouble in terms of TableStats.java; all of the tests in 
TableStats.java worked, except for estimateSelectivity. After narrowing 
down which specifc assert cases, we found that it was the NOT_EQUALS cases
that were not being executed as they should have. To fix this, NOT_EQUALS
was implemented in its own if statement, and its results were returned 
immediately to avoid any miscalculations.

The constructor of TableStats was also a point of hardship. We ended up 
deciding that HashMaps were the best way to design this class, since we 
have used them before in SimpleDB and also they were used in this specific
class prior to us filling our portion of the code. In essence, we found the
transaction ID to get an iterator to help catch exceptions and also to get
values we needed. We start by getting the max and min values in separate 
HashMaps, and then proceed on to the histograms, adding values according to 
if they are INT_TYPE or STRING_TYPE. The histogram is then added to our 
HashMap, and we are able to use the HashMap to find different stats, such 
as selectivity.

This lab took us a bit longer than the other two labs, as the concepts in 
this lab are much more advanced. The detailed nature of this lab, such as 
calculating table statistics, took a lot longer than we expected, hence our
project being late. Overall, the lab took us about a week and a half, almost
2 weeks.

__________________________________________________________________________

LAB 2 REPORT

The implementations of Predicate.java, JoinPredicate.java, and Filter.java
were fairly straightforward. No major design decisions were necessary.

In Join.java we used simple nested loops. For each tuple from the first
child, we checked whether or not it could be joined with each tuple from
the second child.

To implement aggregates, we had to do implement the different classes
for IntegerAggregator and StringAggregator. IntegerAggregator was more 
difficult, as initially we tried to implement only just one HashMap for
both "group by" and "count" operations, but after adding a second HashMap
to split them up the design became a lot easier. By using the two HashMaps,
we easily went through each case (SUM, AVG, MAX, MIN, COUNT) to merge the
groups.

For both Integer and String Aggregator, we created our own function
getTupleDesc(), similar to the one we were required to do in Aggregate.java.
This allowed us to do the same thing in Aggregate.java, which was set the
tuple descriptor for the given tuple. Both Integer and String Aggregator's
iterator were implemented similarly, by systematically adding tuples with
the appropriate tuple descriptor given from getTupleDesc(); the only
difference was in IntegerAggregator we also dealt with the AVG operation.

For Aggregate.java, we followed the design hint of implementing it depending
on if it was going to be an IntegerAggregator or a StringAggregator. We
also broke it up into the child passed in, and an aggregated child that was
initalized and opened when the open() function was called. This allowed us to
implement functions such as fetchNext() more easily, since we already had the
aggregated child when we needed it. Most functions were easy to implement
once we conceptually had the right member variables to work with, but coming
to this conceptual standpoint was rather challenging.

For HeapPage.java and HeapFile.java, we implemented the rest of HeapPage
first since the insertTuple and deleteTuple functions of HeapFile use the
HeapPage functions of the same prototype. Also, most of HeapPage was
previously done from what was given to us and Lab 1, so all we had to
implement was the dirty functions and insert and delete tuple; for the
dirty functions, we used the preexisting member variable _dirty to mark
the tId as dirty or not. By manipulating the i value in the markSlotUsed
function, we either set or cleared the bit value in the member variable
header[] to represent if it was used or not. After that, insert and deleting
tuples just consisted of placing them in the tuples member array, and then
marking or unmarking them accordingly.

For HeapFile.java, it was quite a bit more tricky. Although "not necessary",
we had to implement writePage; despite giving us some initial problems, it
worked once we implemented it similarly to how we did in readPage, just this
time not returning anything. Deleting a tuple wasn't too bad; we just had
to find the page Id and page of that tuple, and delete the tuple. The part
that was almost overlooked was testing to make sure it was a valid tuple to
delete, hence we put the deleting tuple code within an if statement. Inserting
a tuple on the otherhand, was arguably one of the most challenging functions
in this lab. We tried creating an auxiliary function to help, but it ended
adding more complexity than it was worth; what ended up working, though, 
as adding a completely new member variable in m_bufferPool. This allowed
us to access the bufferpool to find the page via the tid. By looping to
find the first such page that had an empty slot, we could insert the tuple,
or otherwise create a new page.

Insert.java and Delete.java were fairly simple to implement because we've
already implemented iteration methods before, and inserting and deleting
tuples calls on the BufferPool methods insertTuple and deleteTuple.

In BufferPool, we implemented flushAllPages by iterating through the
concurrent hash map and calling flushpage on each one. flushPage() writes
the page to disk and marks it as no longer dirty. This is used by the
evictPage() method when the page that we are trying to evict has been modified
from disk. Our evictPage() method simply iterates over the map to find
a page to evict. We could create another index to maintain insertion order
or most recently used order, but since we don't know how to predict
what pages will be read, we decided it would be simpler to iterate and evict
the first page. However, had we wanted to maintain insertion order, a linked
hash map might be useful, except for the fact that multiple threads can use
the concurrent hash map at a time.

Overall the lab took us again about a week, but this time a lot more time
dedicated in the end to fine-tune our functions to get all tests to work,
hence taking an extra day.

However, even though the given tests work, we were unable to solve some
other problems. For example, aggregates do not function correctly unless
GROUP BY is specified, so statements such as
	SELECT COUNT(*) from Data d;
do not work correctly.
